{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House prices\n",
    "\n",
    "Kairos (April 2018)\n",
    "\n",
    "\n",
    "## Description\n",
    "Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n",
    "\n",
    "\n",
    "## Data\n",
    "79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa.\n",
    "\n",
    "## Challenge\n",
    "Predicting the final price of each home.\n",
    "\n",
    "## Method\n",
    "We'll use Tensorflow as out method to develop the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "In this exercise, we'll dive a bit more deeply into the training and evaluation of a model.\n",
    "\n",
    "As in the prior exercises, we're working with the housing data set, to try and predict `SalePrice`.\n",
    "\n",
    "In this exercise, we'll use multiple features (instead of a single feature), and also get familiar with the train / validation / test split methodology.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set Up\n",
    "In this first cell, we'll load the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn import learn_io\n",
    "\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load our data set\n",
    "Next, we'll load our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing_dataframe = pd.read_csv(\"input/cleaned_houses_prices_train.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choosing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_features(housing_dataframe):\n",
    "  \"\"\"Prepares input features from California housing data set.\n",
    "\n",
    "  Args:\n",
    "    housing_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the housing data set.\n",
    "  Returns:\n",
    "    A DataFrame that contains the features to be used for the model, including\n",
    "    synthetic features.\n",
    "  \"\"\"\n",
    "  selected_features = housing_dataframe[\n",
    "    [\"MSZoning\",\n",
    "     \"YearRemodAdd\",\n",
    "     \"Utilities\",\n",
    "     \"Neighborhood\",\n",
    "     \"HouseStyle\",\n",
    "     \"BedroomAbvGr\",\n",
    "     \"OverallCond\"]]\n",
    "  processed_features = selected_features.copy()\n",
    "  # Create a synthetic feature.\n",
    "  processed_features[\"total_floor_sf\"] = (\n",
    "    housing_dataframe[\"1stFlrSF\"] +\n",
    "    housing_dataframe[\"2ndFlrSF\"])\n",
    "  return processed_features\n",
    "\n",
    "def preprocess_targets(housing_dataframe):\n",
    "  \"\"\"Prepares target features (i.e., labels) from housing data set.\n",
    "\n",
    "  Args:\n",
    "    housing_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the housing data set.\n",
    "  Returns:\n",
    "    A DataFrame that contains the target feature.\n",
    "  \"\"\"\n",
    "  output_targets = pd.DataFrame()\n",
    "  # Scale the target to be in units of thousands of dollars.\n",
    "  output_targets[\"SalePrice\"] = (housing_dataframe[\"SalePrice\"] / 1000.0)\n",
    "  return output_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Splitting the dataset into training, test and validation datasets\n",
    "For the training set, we'll choose the first 1000 examples, out of the total of 1451.\n",
    "We'll create an inter examples of 500.\n",
    "Validation data will have 250 of each type.\n",
    "Test data will have 250 of each type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_examples = preprocess_features(housing_dataframe.head(1000))\n",
    "training_targets = preprocess_targets(housing_dataframe.head(1000))\n",
    "inter_examples = preprocess_features(housing_dataframe.tail(500))\n",
    "inter_targets = preprocess_targets(housing_dataframe.tail(500))\n",
    "\n",
    "validation_examples = inter_examples.head(250)\n",
    "validation_targets = inter_targets.head(250)\n",
    "\n",
    "test_examples = inter_examples.tail(250)\n",
    "test_targets = inter_targets.tail(250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examine the data\n",
    "Okay, let's look at the data above. We have several input features that we can use.\n",
    "\n",
    "Take a quick skim over the table of values. Do they pass a quick sanity check?\n",
    "\n",
    "Take a look at the data on your own. Everything look okay? See how many issues you can spot. Don't worry if you don't have a background in statistics; common sense is often enough.\n",
    "\n",
    "After you've had a chance to look over the data yourself, check the solution for some additional thoughts on how to sanity check data.\n",
    "\n",
    "Let's take a close look at two features in particular: latitude and longitude. These are geographical coordinates of the city block in question.\n",
    "\n",
    "This might make a nice visualization â€” let's plot latitude and longitude, and use color to show the median_house_value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing_dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    \"\"\"Trains a linear regression model of multiple features.\n",
    "  \n",
    "    Args:\n",
    "      features: pandas DataFrame of features\n",
    "      targets: pandas DataFrame of targets\n",
    "      batch_size: Size of batches to be passed to the model\n",
    "      shuffle: True or False. Whether to shuffle the data.\n",
    "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
    "    Returns:\n",
    "      Tuple of (features, labels) for next data batch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert pandas data into a dict of np arrays.\n",
    "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
    " \n",
    "    # Construct a dataset, and configure batching/repeating.\n",
    "    ds = tf.data.Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    \n",
    "    # Shuffle the data, if specified.\n",
    "    if shuffle:\n",
    "      ds = ds.shuffle(10000)\n",
    "    \n",
    "    # Return the next batch of data.\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_feature_columns(input_features):\n",
    "  \"\"\"Construct the TensorFlow Feature Columns.\n",
    "\n",
    "  Args:\n",
    "    input_features: The names of the numerical input features to use.\n",
    "  Returns:\n",
    "    A set of feature columns\n",
    "  \"\"\" \n",
    "  return set([tf.feature_column.numeric_column(my_feature)\n",
    "              for my_feature in input_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "  \"\"\"Trains a linear regression model of one feature.\n",
    "  \n",
    "  In addition to training, this function also prints training progress information,\n",
    "  as well as a plot of the training and validation loss over time.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: A `float`, the learning rate.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    training_examples: A `DataFrame` containing one or more columns from\n",
    "      `housing_dataframe` to use as input features for training.\n",
    "    training_targets: A `DataFrame` containing exactly one column from\n",
    "      `housing_dataframe` to use as target for training.\n",
    "    validation_examples: A `DataFrame` containing one or more columns from\n",
    "      `housing_dataframe` to use as input features for validation.\n",
    "    validation_targets: A `DataFrame` containing exactly one column from\n",
    "      `housing_dataframe` to use as target for validation.\n",
    "      \n",
    "  Returns:\n",
    "    A `LinearRegressor` object trained on the training data.\n",
    "  \"\"\"\n",
    "\n",
    "  periods = 10\n",
    "  steps_per_period = steps / periods\n",
    "\n",
    "  # Create input functions\n",
    "  training_input_fn = learn_io.pandas_input_fn(\n",
    "     x=training_examples, y=training_targets[\"SalePrice\"],\n",
    "     num_epochs=None, batch_size=batch_size)\n",
    "  predict_training_input_fn = learn_io.pandas_input_fn(\n",
    "     x=training_examples, y=training_targets[\"SalePrice\"],\n",
    "     num_epochs=1, shuffle=False)\n",
    "  predict_validation_input_fn = learn_io.pandas_input_fn(\n",
    "      x=validation_examples, y=validation_targets[\"SalePrice\"],\n",
    "      num_epochs=1, shuffle=False)\n",
    "\n",
    "  # Create a linear regressor object.\n",
    "  feature_columns = set([tf.contrib.layers.real_valued_column(my_feature) for my_feature in training_examples])\n",
    "  linear_regressor = tf.contrib.learn.LinearRegressor(\n",
    "      feature_columns=feature_columns,\n",
    "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate),\n",
    "      gradient_clip_norm=5.0\n",
    "  )\n",
    "\n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print \"Training model...\"\n",
    "  print \"RMSE (on training data):\"\n",
    "  training_rmse = []\n",
    "  validation_rmse = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    linear_regressor.fit(\n",
    "        input_fn=training_input_fn,\n",
    "        steps=steps_per_period,\n",
    "    )\n",
    "    # Take a break and compute predictions.\n",
    "    training_predictions = list(linear_regressor.predict(input_fn=predict_training_input_fn))\n",
    "    validation_predictions = list(linear_regressor.predict(input_fn=predict_validation_input_fn))\n",
    "    # Compute training and validation loss.\n",
    "    training_root_mean_squared_error = math.sqrt(\n",
    "        metrics.mean_squared_error(training_predictions, training_targets))\n",
    "    validation_root_mean_squared_error = math.sqrt(\n",
    "        metrics.mean_squared_error(validation_predictions, validation_targets))\n",
    "    # Occasionally print the current loss.\n",
    "    print \"  period %02d : %0.2f\" % (period, training_root_mean_squared_error)\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    training_rmse.append(training_root_mean_squared_error)\n",
    "    validation_rmse.append(validation_root_mean_squared_error)\n",
    "  print \"Model training finished.\"\n",
    "\n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.ylabel(\"RMSE\")\n",
    "  plt.xlabel(\"Periods\")\n",
    "  plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "  plt.tight_layout()\n",
    "  plt.plot(training_rmse, label=\"training\")\n",
    "  plt.plot(validation_rmse, label=\"validation\")\n",
    "  plt.legend()\n",
    "\n",
    "  return linear_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_regressor = train_model(\n",
    "    learning_rate=0.0001,\n",
    "    steps=350,\n",
    "    batch_size=5,\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate on Test Data\n",
    "In the cell below, load in the test data set and evaluate your model on it.\n",
    "\n",
    "We've done a lot of iteration on our validation data. Let's make sure we haven't overfit to the pecularities of that particular sample.\n",
    "\n",
    "Test data set is located input/test.cvs.\n",
    "\n",
    "How does your test performance compare to the validation performance? What does this say about the generalization performance of your model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_examples.info()\n",
    "#test_targets.info()\n",
    "predict_test_input_fn = lambda: my_input_fn(\n",
    "      test_examples, \n",
    "      test_targets[\"SalePrice\"], \n",
    "      num_epochs=1, \n",
    "      shuffle=False)\n",
    "\n",
    "test_predictions = linear_regressor.predict(input_fn=predict_test_input_fn)\n",
    "test_predictions = np.array([item['predictions'][0] for item in test_predictions])\n",
    "\n",
    "root_mean_squared_error = math.sqrt(\n",
    "    metrics.mean_squared_error(test_predictions, test_targets))\n",
    "\n",
    "print \"Final RMSE (on test data): %0.2f\" % root_mean_squared_error\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
